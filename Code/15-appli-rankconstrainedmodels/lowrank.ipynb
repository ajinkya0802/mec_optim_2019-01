{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to estimate our low rank affinity matrix. We will look to perform proximal gradient descent with nuclear norm regularization, to find the low rank affinity matrix that best approximates the matching in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity = function(Xvals, Yvals, sigma = 1, lambda = 1) {\n",
    "    phis = kronecker(t(Yvals), t(Xvals))\n",
    "    dX = dim(Xvals)[2]\n",
    "    dY = dim(Yvals)[2]\n",
    "    n = dim(Xvals)[1]\n",
    "    if (n != dim(Yvals)[1]) {\n",
    "        stop(\"Dimensions of Xvals and Yvals do not match.\")\n",
    "    }\n",
    "    \n",
    "    p = rep(1/n, n)\n",
    "    q = rep(1/n, n)\n",
    "    IX = rep(1, n)\n",
    "    tIY = matrix(rep(1, n), nrow = 1)\n",
    "    f = p %*% tIY\n",
    "    g = IX %*% t(q)\n",
    "    pihat = diag(n)/n\n",
    "    v = rep(0, n)\n",
    "    \n",
    "    A = rep(0, dX * dY)\n",
    "    t_k = 0.3  # step size for the prox grad algorithm (or grad descent when lambda=0)\n",
    "    \n",
    "    iterCount = 0\n",
    "    \n",
    "    while (1) {\n",
    "        # Compute pi_A\n",
    "        Phi = Xvals %*% matrix(A, nrow = dX) %*% t(Yvals)\n",
    "        contIpfp = TRUE\n",
    "        iterIpfp = 0\n",
    "        while (contIpfp) {\n",
    "            iterIpfp = iterIpfp + 1\n",
    "            u = sigma * log(apply(g * exp((Phi - IX %*% t(v))/sigma), 1, sum))\n",
    "            vnext = sigma * log(apply(f * exp((Phi - u %*% tIY)/sigma), 2, sum))\n",
    "            error = max(abs(apply(g * exp((Phi - IX %*% t(vnext) - u %*% tIY)/sigma), \n",
    "                1, sum) - 1))\n",
    "            if ((error < tolIpfp) | (iterIpfp >= maxiterIpfp)) {\n",
    "                contIpfp = FALSE\n",
    "            }\n",
    "            v = vnext\n",
    "        }\n",
    "        \n",
    "        pi = f * g * exp((Phi - IX %*% t(v) - u %*% tIY)/sigma)\n",
    "        \n",
    "        if (iterIpfp >= maxiterIpfp) {\n",
    "            stop(\"maximum number of iterations reached\")\n",
    "        }\n",
    "        \n",
    "        # do prox grad descent\n",
    "        thegrad = c(phis %*% c(pi - pihat))\n",
    "        \n",
    "        # take one gradient step\n",
    "        A = A - t_k * thegrad\n",
    "        \n",
    "        if (lambda > 0) \n",
    "            {\n",
    "                # compute the proximal operator\n",
    "                SVD = svd(matrix(A, nrow = dX))\n",
    "                U = SVD$u\n",
    "                D = SVD$d\n",
    "                V = SVD$v\n",
    "                \n",
    "                D = pmax(D - lambda * t_k, 0)\n",
    "                A = c(U %*% diag(D) %*% t(V))\n",
    "            }  # if lambda = 0 then we are just taking one step of gradient descent\n",
    "        \n",
    "        \n",
    "        ### testing optimality\n",
    "        if (iterCount%%10 == 0) {\n",
    "            alpha = 1\n",
    "            tmp = svd(matrix(A - alpha * thegrad, nrow = dX))\n",
    "            tmp_second = sum((A - c(tmp$u %*% diag(pmax(tmp$d - alpha * lambda, 0)) %*% \n",
    "                t(tmp$v)))^2)\n",
    "            cat(\"testing optimality \", tmp_second, \"\\n\")\n",
    "        }\n",
    "        \n",
    "        if (lambda > 0) {\n",
    "            theval = sum(thegrad * c(A)) - sigma * sum(pi * log(pi)) + lambda * sum(D)\n",
    "        } else {\n",
    "            theval = sum(thegrad * c(A)) - sigma * sum(pi * log(pi))\n",
    "        }\n",
    "        \n",
    "        iterCount = iterCount + 1\n",
    "        \n",
    "        if (iterCount > 1 && abs(theval - theval_old) < 1e-06) {\n",
    "            break\n",
    "        }\n",
    "        theval_old = theval   \n",
    "    }\n",
    "    return(list(A = matrix(A, nrow = dX), val = theval))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute this for a fixed $\\lambda$. We could vary the value of $\\lambda$ using cross-validation to get the desired level of rank reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing optimality  0.09042313 \n",
      "testing optimality  0.0001098681 \n"
     ]
    }
   ],
   "source": [
    "mydata <- read.csv(\"DGS_low_rank_april16.csv\")\n",
    "#Xvals = mydata[,c(1:22, 45:48)]\n",
    "#Yvals = mydata[,c(23:44, 56:59)]\n",
    "\n",
    "Xvals = mydata[,c(45:48)]\n",
    "Yvals = mydata[,c(56:59)]\n",
    "tolIpfp = 1e-12\n",
    "maxiterIpfp = 1000\n",
    "\n",
    "seed = 777\n",
    "set.seed(seed)\n",
    "\n",
    "# Standardize\n",
    "meanX = apply(Xvals, 2, mean)\n",
    "meanY = apply(Yvals, 2, mean)\n",
    "sdX = apply(Xvals, 2, sd)\n",
    "sdY = apply(Yvals, 2, sd)\n",
    "\n",
    "Xvals = t(t(Xvals) - meanX)\n",
    "Yvals = t(t(Yvals) - meanY)\n",
    "Xvals = t(t(Xvals)/sdX)\n",
    "Yvals = t(t(Yvals)/sdY)\n",
    "\n",
    "res = affinity(Xvals, Yvals, sigma=1, lambda=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td> 0.259367257</td><td> 0.019745466</td><td>-0.06198573 </td><td> 0.002713854</td></tr>\n",
       "\t<tr><td> 0.029096742</td><td> 0.002923766</td><td>-0.01160270 </td><td> 0.003663925</td></tr>\n",
       "\t<tr><td>-0.054458621</td><td>-0.007945686</td><td> 0.03794256 </td><td>-0.018583367</td></tr>\n",
       "\t<tr><td>-0.009374241</td><td> 0.003116353</td><td>-0.02288552 </td><td> 0.018058736</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{llll}\n",
       "\t  0.259367257 &  0.019745466 & -0.06198573  &  0.002713854\\\\\n",
       "\t  0.029096742 &  0.002923766 & -0.01160270  &  0.003663925\\\\\n",
       "\t -0.054458621 & -0.007945686 &  0.03794256  & -0.018583367\\\\\n",
       "\t -0.009374241 &  0.003116353 & -0.02288552  &  0.018058736\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "|  0.259367257 |  0.019745466 | -0.06198573  |  0.002713854 |\n",
       "|  0.029096742 |  0.002923766 | -0.01160270  |  0.003663925 |\n",
       "| -0.054458621 | -0.007945686 |  0.03794256  | -0.018583367 |\n",
       "| -0.009374241 |  0.003116353 | -0.02288552  |  0.018058736 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]         [,2]         [,3]        [,4]        \n",
       "[1,]  0.259367257  0.019745466 -0.06198573  0.002713854\n",
       "[2,]  0.029096742  0.002923766 -0.01160270  0.003663925\n",
       "[3,] -0.054458621 -0.007945686  0.03794256 -0.018583367\n",
       "[4,] -0.009374241  0.003116353 -0.02288552  0.018058736"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = res$A\n",
    "val = res$val\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "2"
      ],
      "text/latex": [
       "2"
      ],
      "text/markdown": [
       "2"
      ],
      "text/plain": [
       "[1] 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qr(A)$rank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
